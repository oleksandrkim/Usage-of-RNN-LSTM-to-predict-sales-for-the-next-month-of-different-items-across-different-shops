{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of RNN LSTM to predict sales for the next month of different items across different shops on the basis of historical data of sales for last 34 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the project for **Kaggle** competion. The data was provided by 1C company. <br>\n",
    "Competition itself:  https://www.kaggle.com/c/competitive-data-science-predict-future-sales <br>\n",
    "Total number of items: **21 800** <br>\n",
    "Total number of sales: **2 935 849** <br>\n",
    "Number of shops: **60**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea is to create a RNN LSTM model for every item for every shop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = pd.read_csv('sales_train_v2.csv')\n",
    "df_shops = pd.read_csv('shops.csv')\n",
    "df_it_cat = pd.read_csv('item_categories.csv')\n",
    "df_items = pd.read_csv('items.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_sales.merge(df_shops,on='shop_id').merge(df_items,on='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop uneeded columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(['date', 'shop_name', 'city', 'item_name','item_category_id'], axis=1, inplace=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sales of each item per store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this example a price of the first 3000 items of a shop number 2 is predicted**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN LSTM** is used to predict price of each item in the shop. A separate neural network is created for every item <br>\n",
    "To predict the sales of the next month - data of the previous six month are taken, even though in the beginning AI was trained on a data of almost three years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing I noticed that the less number of neurons in layers tends to produce prediction that are closer to real value, so I set number of neuron at 50<br> \n",
    "Number of neurons to **50** <br>\n",
    "Number of layers: 4 LSTM layers <br>\n",
    "Optimizer: Adam <br>\n",
    "Loss calculation: MSE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "for kk in range(1): #can be changed if more than 1 shop is processed in one run\n",
    "    df_shop = df_merged.loc[df_merged['shop_id'] == 2]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #pivot\n",
    "    df_p = pd.pivot_table(df_shop,index=[\"date_block_num\"],values=[\"item_cnt_day\"], columns=[\"item_id\"],aggfunc=[np.sum])\n",
    "    \n",
    "    #replace nan with \n",
    "    df_p = df_p.fillna(0)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    col_names = list(df_p.columns.values)\n",
    "    col_list=[]\n",
    "    for col in col_names:\n",
    "        col_list.append(col[2])\n",
    "\n",
    "    pred_list_zero=[]\n",
    "    for ppr in range(0,3000):   #range(len(col_names)):\n",
    "        id_1 = df_p.iloc[:, ppr].values\n",
    "        \n",
    "        id_1 = id_1.reshape(-1, 1)\n",
    "        \n",
    "        # Feature Scaling\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        sc = MinMaxScaler(feature_range = (0, 1))\n",
    "        training_set_scaled = sc.fit_transform(id_1)\n",
    "        \n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for i in range(6, 34):\n",
    "            X_train.append(training_set_scaled[i-6:i, 0])\n",
    "            y_train.append(training_set_scaled[i, 0])\n",
    "        X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "        \n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "        \n",
    "        regressor = Sequential()\n",
    "        \n",
    "        # Adding the first LSTM layer and some Dropout regularisation\n",
    "        regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "        regressor.add(Dropout(0.2))\n",
    "        \n",
    "        # Adding a second LSTM layer and some Dropout regularisation\n",
    "        regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "        regressor.add(Dropout(0.2))\n",
    "        \n",
    "        # Adding a third LSTM layer and some Dropout regularisation\n",
    "        regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "        regressor.add(Dropout(0.2))\n",
    "        \n",
    "        # Adding a fourth LSTM layer and some Dropout regularisation\n",
    "        regressor.add(LSTM(units = 50))\n",
    "        regressor.add(Dropout(0.2))\n",
    "        \n",
    "        # Adding the output layer\n",
    "        regressor.add(Dense(units = 1))\n",
    "        \n",
    "        regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
    "        \n",
    "        X_test = []\n",
    "        for i in range(33, 34): \n",
    "            X_test.append(training_set_scaled[i-6:i, 0])\n",
    "        X_test = np.array(X_test)\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "        \n",
    "        predicted_sales = regressor.predict(X_test)\n",
    "        predicted_sales_reverse = sc.inverse_transform(predicted_sales) #return back\n",
    "        pred_list_zero.append(predicted_sales_reverse[0,0])\n",
    "    \n",
    "        print(\"Id \",ppr,\" out of \", len(col_names))\n",
    "        print(\"##/##/##/##/##/##/\")\n",
    "        K.clear_session()\n",
    "        \n",
    "    df_lists = [('Item_id', col_list), ('predictions', pred_list_zero)]\n",
    "    df_to_csv = pd.DataFrame.from_items(df_lists)\n",
    "\n",
    "    #predictions are saved in a csv file\n",
    "\n",
    "    df_to_csv.to_csv('file_'+str(kk)+'.csv', sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_id</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>1511</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>1512</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>1513</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>1514</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1515</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>1516</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>1523</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>1524</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>1527</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>1530</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>1534</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>1535</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>1537</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>1538</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>1539</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>1540</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1541</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>1542</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>1544</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>1545</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>1547</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>1548</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>1549</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>1555</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>1556</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>1557</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>1560</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>1563</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>1564</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>1565</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>1568</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>1569</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>1570</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>1571</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>1572</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>1578</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1586</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>1598</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1602</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>1673</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>1701</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1727</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>1729</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1744</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>1782</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1783</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Item_id  predictions\n",
       "200     1511          1.0\n",
       "201     1512          1.0\n",
       "202     1513          0.0\n",
       "203     1514          0.0\n",
       "204     1515          0.0\n",
       "205     1516          2.0\n",
       "206     1523          1.0\n",
       "207     1524         -0.0\n",
       "208     1527          0.0\n",
       "209     1530          0.0\n",
       "210     1534          1.0\n",
       "211     1535          1.0\n",
       "212     1537          0.0\n",
       "213     1538          0.0\n",
       "214     1539          1.0\n",
       "215     1540          3.0\n",
       "216     1541          0.0\n",
       "217     1542          0.0\n",
       "218     1544         -0.0\n",
       "219     1545          0.0\n",
       "220     1547         -0.0\n",
       "221     1548          0.0\n",
       "222     1549          0.0\n",
       "223     1555          2.0\n",
       "224     1556          3.0\n",
       "225     1557          0.0\n",
       "226     1560          1.0\n",
       "227     1563          2.0\n",
       "228     1564          2.0\n",
       "229     1565          1.0\n",
       "230     1568          1.0\n",
       "231     1569          1.0\n",
       "232     1570          1.0\n",
       "233     1571          1.0\n",
       "234     1572          0.0\n",
       "235     1578          0.0\n",
       "236     1579          0.0\n",
       "237     1583          0.0\n",
       "238     1586          0.0\n",
       "239     1597          0.0\n",
       "240     1598          0.0\n",
       "241     1602          0.0\n",
       "242     1673          0.0\n",
       "243     1701          0.0\n",
       "244     1718          0.0\n",
       "245     1727          0.0\n",
       "246     1729          0.0\n",
       "247     1744          0.0\n",
       "248     1782         -0.0\n",
       "249     1783          0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preds = pd.read_csv('D:/viburnum/1C predict future sales/shop2/file_2_0-3000.csv') \n",
    "df_preds['predictions'] = df_preds['predictions'].round(0)\n",
    "\n",
    "df_preds.iloc[200:250, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of zeros because these items are sold very rarely or they were popular several years ago, but they are are not present"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
