{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of RNN LSTM to predict sales for the next month of different items across different shops on the basis of historical data of sales for last 34 months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the project for **Kaggle** competion. The data was provided by 1C company. <br>\n",
    "Competition itself:  https://www.kaggle.com/c/competitive-data-science-predict-future-sales <br>\n",
    "Total number of items: **21 800** <br>\n",
    "Total number of sales: **2 935 849** <br>\n",
    "Number of shops: **60**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea is to create a RNN LSTM model for every item for every shop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = pd.read_csv('sales_train_v2.csv')\n",
    "df_shops = pd.read_csv('shops.csv')\n",
    "df_it_cat = pd.read_csv('item_categories.csv')\n",
    "df_items = pd.read_csv('items.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_sales.merge(df_shops,on='shop_id').merge(df_items,on='item_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop uneeded columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.drop(['date', 'shop_name', 'city', 'item_name','item_category_id'], axis=1, inplace=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict sales of each item per store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this example a price of a shop number 2 is predicted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "for kk in range(1):\n",
    "    df_shop = df_merged.loc[df_merged['shop_id'] == 2]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #pivot\n",
    "    df_p = pd.pivot_table(df_shop,index=[\"date_block_num\"],values=[\"item_cnt_day\"], columns=[\"item_id\"],aggfunc=[np.sum])\n",
    "    \n",
    "    #replace nan with \n",
    "    df_p = df_p.fillna(0)\n",
    "    \n",
    "    #######\n",
    "    \n",
    "    col_names = list(df_p.columns.values)\n",
    "    col_list=[]\n",
    "    for col in col_names:\n",
    "        col_list.append(col[2])\n",
    "    \n",
    "    \n",
    "    #layers, neurons, epochs, batch size\n",
    "    #pred_list (4/50/100/32)\n",
    "    #pred_list2(4/256/15/6) worse?\n",
    "    #pred_list3(4/128/25/24) better?\n",
    "    #pred_list4(4/128/25/40) better?\n",
    "    #pred_list5(4/64/25/32) better?\n",
    "    #pred_list6 (4/50/200/32)\n",
    "    pred_list_zero=[]\n",
    "    for ppr in range(2000,3000):   #range(len(col_names)):\n",
    "        id_1 = df_p.iloc[:, ppr].values\n",
    "        \n",
    "        id_1 = id_1.reshape(-1, 1)\n",
    "        \n",
    "        # Feature Scaling\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        sc = MinMaxScaler(feature_range = (0, 1))\n",
    "        training_set_scaled = sc.fit_transform(id_1)\n",
    "        \n",
    "        X_train = []\n",
    "        y_train = []\n",
    "        for i in range(6, 34):\n",
    "            X_train.append(training_set_scaled[i-6:i, 0])\n",
    "            y_train.append(training_set_scaled[i, 0])\n",
    "        X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "        \n",
    "        X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialising the RNN\n",
    "        regressor = Sequential()\n",
    "        \n",
    "        # Adding the first LSTM layer and some Dropout regularisation\n",
    "        #units - cells\n",
    "        #return_sequences - stacked lstm layers\n",
    "        regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "        regressor.add(Dropout(0.2))\n",
    "        \n",
    "        # Adding a second LSTM layer and some Dropout regularisation\n",
    "        regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "        regressor.add(Dropout(0.2))\n",
    "        \n",
    "        # Adding a third LSTM layer and some Dropout regularisation\n",
    "        regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "        regressor.add(Dropout(0.2))\n",
    "        \n",
    "        # Adding a fourth LSTM layer and some Dropout regularisation\n",
    "        regressor.add(LSTM(units = 50))\n",
    "        regressor.add(Dropout(0.2))\n",
    "        \n",
    "        # Adding the output layer\n",
    "        regressor.add(Dense(units = 1))\n",
    "        \n",
    "        # Compiling the RNN\n",
    "        regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "        \n",
    "        # Fitting the RNN to the Training set\n",
    "        regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        X_test = []\n",
    "        for i in range(33, 34): \n",
    "            X_test.append(training_set_scaled[i-6:i, 0])\n",
    "        X_test = np.array(X_test)\n",
    "        X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "        \n",
    "        \n",
    "        predicted_sales = regressor.predict(X_test)\n",
    "        predicted_sales_reverse = sc.inverse_transform(predicted_sales) #return back\n",
    "        pred_list_zero.append(predicted_sales_reverse[0,0])\n",
    "        #item_id = col_names[ppr][2]\n",
    "        \n",
    "        #dict_id_sales[item_id] = predicted_sales_reverse\n",
    "        #dict_id_shops[kk] = dict_id_sales\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"Id \",ppr,\" out of \", len(col_names))\n",
    "        print(\"##/##/##/##/##/##/\")\n",
    "        K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
